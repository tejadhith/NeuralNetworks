{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Neural Network\n",
    "class SqrtNet(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(SqrtNet, self).__init__()\n",
    "        # Fully Connected Layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        x = torch.sqrt(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 100, 10]              20\n",
      "              ReLU-2              [-1, 100, 10]               0\n",
      "            Linear-3              [-1, 100, 10]             110\n",
      "              ReLU-4              [-1, 100, 10]               0\n",
      "            Linear-5               [-1, 100, 1]              11\n",
      "================================================================\n",
      "Total params: 141\n",
      "Trainable params: 141\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initializing the Neural Network\n",
    "model = SqrtNet()\n",
    "# Visualizing the Neural Network\n",
    "summary(model, (100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Defining Loss Function\n",
    "criterion = nn.MSELoss()\n",
    "# Defining Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "# b = torch.randint(high = 100, low = 0, size = (100, 1), dtype = torch.int)\n",
    "# b = torch.randn((1000, 1))\n",
    "# a = torch.pow(b, 2)\n",
    "a = torch.tensor([[4.0], [9.0], [16.0], [25.0], [36.0], [49.0], [64.0], [81.0], [100.0], [225.0], [256], [900], [1600], [625], [441]])\n",
    "b = torch.tensor([[2.0], [3.0], [4.0], [5.0], [6.0], [7.0], [8.0], [9.0], [10.0], [15], [16], [30], [40], [25], [21]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Defining Train Loop\n",
    "def Train(Input, Target, EPOCH) :\n",
    "    for index in range(EPOCH) :\n",
    "        output = model(Input)\n",
    "        loss = criterion(output, Target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch : {index} | Loss : {loss.item()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 | Loss : 247.91514587402344\n",
      "Epoch : 1 | Loss : 246.7203826904297\n",
      "Epoch : 2 | Loss : 245.5254669189453\n",
      "Epoch : 3 | Loss : 244.33041381835938\n",
      "Epoch : 4 | Loss : 243.13441467285156\n",
      "Epoch : 5 | Loss : 241.9354705810547\n",
      "Epoch : 6 | Loss : 240.73606872558594\n",
      "Epoch : 7 | Loss : 239.53646850585938\n",
      "Epoch : 8 | Loss : 238.33673095703125\n",
      "Epoch : 9 | Loss : 237.13693237304688\n",
      "Epoch : 10 | Loss : 235.93600463867188\n",
      "Epoch : 11 | Loss : 234.7335968017578\n",
      "Epoch : 12 | Loss : 233.53067016601562\n",
      "Epoch : 13 | Loss : 232.32537841796875\n",
      "Epoch : 14 | Loss : 231.1161651611328\n",
      "Epoch : 15 | Loss : 229.9016876220703\n",
      "Epoch : 16 | Loss : 228.67811584472656\n",
      "Epoch : 17 | Loss : 227.45103454589844\n",
      "Epoch : 18 | Loss : 226.2151336669922\n",
      "Epoch : 19 | Loss : 224.97242736816406\n",
      "Epoch : 20 | Loss : 223.71563720703125\n",
      "Epoch : 21 | Loss : 222.45480346679688\n",
      "Epoch : 22 | Loss : 221.189697265625\n",
      "Epoch : 23 | Loss : 219.92051696777344\n",
      "Epoch : 24 | Loss : 218.64727783203125\n",
      "Epoch : 25 | Loss : 217.35885620117188\n",
      "Epoch : 26 | Loss : 216.0559844970703\n",
      "Epoch : 27 | Loss : 214.74801635742188\n",
      "Epoch : 28 | Loss : 213.4077606201172\n",
      "Epoch : 29 | Loss : 212.05767822265625\n",
      "Epoch : 30 | Loss : 210.67208862304688\n",
      "Epoch : 31 | Loss : 209.26284790039062\n",
      "Epoch : 32 | Loss : 207.84571838378906\n",
      "Epoch : 33 | Loss : 206.34300231933594\n",
      "Epoch : 34 | Loss : 204.822998046875\n",
      "Epoch : 35 | Loss : 203.29234313964844\n",
      "Epoch : 36 | Loss : 201.75096130371094\n",
      "Epoch : 37 | Loss : 200.1990509033203\n",
      "Epoch : 38 | Loss : 198.63723754882812\n",
      "Epoch : 39 | Loss : 197.06591796875\n",
      "Epoch : 40 | Loss : 195.48538208007812\n",
      "Epoch : 41 | Loss : 193.89614868164062\n",
      "Epoch : 42 | Loss : 192.29855346679688\n",
      "Epoch : 43 | Loss : 190.69284057617188\n",
      "Epoch : 44 | Loss : 189.07925415039062\n",
      "Epoch : 45 | Loss : 187.45797729492188\n",
      "Epoch : 46 | Loss : 185.82928466796875\n",
      "Epoch : 47 | Loss : 184.19334411621094\n",
      "Epoch : 48 | Loss : 182.55030822753906\n",
      "Epoch : 49 | Loss : 180.90040588378906\n",
      "Epoch : 50 | Loss : 179.24386596679688\n",
      "Epoch : 51 | Loss : 177.58062744140625\n",
      "Epoch : 52 | Loss : 175.910888671875\n",
      "Epoch : 53 | Loss : 174.23480224609375\n",
      "Epoch : 54 | Loss : 172.552490234375\n",
      "Epoch : 55 | Loss : 170.86427307128906\n",
      "Epoch : 56 | Loss : 169.17013549804688\n",
      "Epoch : 57 | Loss : 167.47015380859375\n",
      "Epoch : 58 | Loss : 165.7642364501953\n",
      "Epoch : 59 | Loss : 164.05258178710938\n",
      "Epoch : 60 | Loss : 162.3356475830078\n",
      "Epoch : 61 | Loss : 160.61331176757812\n",
      "Epoch : 62 | Loss : 158.8859405517578\n",
      "Epoch : 63 | Loss : 157.15347290039062\n",
      "Epoch : 64 | Loss : 155.41635131835938\n",
      "Epoch : 65 | Loss : 153.67462158203125\n",
      "Epoch : 66 | Loss : 151.92849731445312\n",
      "Epoch : 67 | Loss : 150.1782684326172\n",
      "Epoch : 68 | Loss : 148.42398071289062\n",
      "Epoch : 69 | Loss : 146.66586303710938\n",
      "Epoch : 70 | Loss : 144.90428161621094\n",
      "Epoch : 71 | Loss : 143.13917541503906\n",
      "Epoch : 72 | Loss : 141.37081909179688\n",
      "Epoch : 73 | Loss : 139.5994110107422\n",
      "Epoch : 74 | Loss : 137.82522583007812\n",
      "Epoch : 75 | Loss : 136.04830932617188\n",
      "Epoch : 76 | Loss : 134.26873779296875\n",
      "Epoch : 77 | Loss : 132.48704528808594\n",
      "Epoch : 78 | Loss : 130.70413208007812\n",
      "Epoch : 79 | Loss : 128.92030334472656\n",
      "Epoch : 80 | Loss : 127.13544464111328\n",
      "Epoch : 81 | Loss : 125.34971618652344\n",
      "Epoch : 82 | Loss : 123.5634765625\n",
      "Epoch : 83 | Loss : 121.77771759033203\n",
      "Epoch : 84 | Loss : 119.99244689941406\n",
      "Epoch : 85 | Loss : 118.20807647705078\n",
      "Epoch : 86 | Loss : 116.4254150390625\n",
      "Epoch : 87 | Loss : 114.64329528808594\n",
      "Epoch : 88 | Loss : 112.86396789550781\n",
      "Epoch : 89 | Loss : 111.0865707397461\n",
      "Epoch : 90 | Loss : 109.31196594238281\n",
      "Epoch : 91 | Loss : 107.54196166992188\n",
      "Epoch : 92 | Loss : 105.77486419677734\n",
      "Epoch : 93 | Loss : 104.01100158691406\n",
      "Epoch : 94 | Loss : 102.25090026855469\n",
      "Epoch : 95 | Loss : 100.49427032470703\n",
      "Epoch : 96 | Loss : 98.74118041992188\n",
      "Epoch : 97 | Loss : 96.992919921875\n",
      "Epoch : 98 | Loss : 95.2499771118164\n",
      "Epoch : 99 | Loss : 93.5128173828125\n",
      "Epoch : 100 | Loss : 91.78185272216797\n",
      "Epoch : 101 | Loss : 90.05738067626953\n",
      "Epoch : 102 | Loss : 88.3378677368164\n",
      "Epoch : 103 | Loss : 86.62577819824219\n",
      "Epoch : 104 | Loss : 84.92168426513672\n",
      "Epoch : 105 | Loss : 83.22602844238281\n",
      "Epoch : 106 | Loss : 81.53907012939453\n",
      "Epoch : 107 | Loss : 79.85863494873047\n",
      "Epoch : 108 | Loss : 78.18798065185547\n",
      "Epoch : 109 | Loss : 76.52757263183594\n",
      "Epoch : 110 | Loss : 74.87793731689453\n",
      "Epoch : 111 | Loss : 73.2395248413086\n",
      "Epoch : 112 | Loss : 71.61289978027344\n",
      "Epoch : 113 | Loss : 69.99849700927734\n",
      "Epoch : 114 | Loss : 68.39683532714844\n",
      "Epoch : 115 | Loss : 66.80836486816406\n",
      "Epoch : 116 | Loss : 65.23369598388672\n",
      "Epoch : 117 | Loss : 63.67324447631836\n",
      "Epoch : 118 | Loss : 62.12740707397461\n",
      "Epoch : 119 | Loss : 60.596736907958984\n",
      "Epoch : 120 | Loss : 59.0816764831543\n",
      "Epoch : 121 | Loss : 57.58258056640625\n",
      "Epoch : 122 | Loss : 56.094966888427734\n",
      "Epoch : 123 | Loss : 54.621925354003906\n",
      "Epoch : 124 | Loss : 53.1602783203125\n",
      "Epoch : 125 | Loss : 51.71302795410156\n",
      "Epoch : 126 | Loss : 50.2833137512207\n",
      "Epoch : 127 | Loss : 48.8717041015625\n",
      "Epoch : 128 | Loss : 47.478782653808594\n",
      "Epoch : 129 | Loss : 46.10502243041992\n",
      "Epoch : 130 | Loss : 44.75088119506836\n",
      "Epoch : 131 | Loss : 43.40460205078125\n",
      "Epoch : 132 | Loss : 42.075382232666016\n",
      "Epoch : 133 | Loss : 40.766605377197266\n",
      "Epoch : 134 | Loss : 39.47083282470703\n",
      "Epoch : 135 | Loss : 38.18130874633789\n",
      "Epoch : 136 | Loss : 36.91326904296875\n",
      "Epoch : 137 | Loss : 35.65425109863281\n",
      "Epoch : 138 | Loss : 34.39739990234375\n",
      "Epoch : 139 | Loss : 33.16328048706055\n",
      "Epoch : 140 | Loss : 31.952409744262695\n",
      "Epoch : 141 | Loss : 30.719440460205078\n",
      "Epoch : 142 | Loss : 29.4959659576416\n",
      "Epoch : 143 | Loss : 28.296998977661133\n",
      "Epoch : 144 | Loss : 27.123046875\n",
      "Epoch : 145 | Loss : 25.9748592376709\n",
      "Epoch : 146 | Loss : 24.853181838989258\n",
      "Epoch : 147 | Loss : 23.75873374938965\n",
      "Epoch : 148 | Loss : 22.693044662475586\n",
      "Epoch : 149 | Loss : 21.655681610107422\n",
      "Epoch : 150 | Loss : 20.647113800048828\n",
      "Epoch : 151 | Loss : 19.667701721191406\n",
      "Epoch : 152 | Loss : 18.717748641967773\n",
      "Epoch : 153 | Loss : 17.797466278076172\n",
      "Epoch : 154 | Loss : 16.907047271728516\n",
      "Epoch : 155 | Loss : 16.04655647277832\n",
      "Epoch : 156 | Loss : 15.216045379638672\n",
      "Epoch : 157 | Loss : 14.415471076965332\n",
      "Epoch : 158 | Loss : 13.644769668579102\n",
      "Epoch : 159 | Loss : 12.903788566589355\n",
      "Epoch : 160 | Loss : 12.192351341247559\n",
      "Epoch : 161 | Loss : 11.510190963745117\n",
      "Epoch : 162 | Loss : 10.857036590576172\n",
      "Epoch : 163 | Loss : 10.232540130615234\n",
      "Epoch : 164 | Loss : 9.636316299438477\n",
      "Epoch : 165 | Loss : 9.067933082580566\n",
      "Epoch : 166 | Loss : 8.526924133300781\n",
      "Epoch : 167 | Loss : 8.012767791748047\n",
      "Epoch : 168 | Loss : 7.524926662445068\n",
      "Epoch : 169 | Loss : 7.0628132820129395\n",
      "Epoch : 170 | Loss : 6.625796794891357\n",
      "Epoch : 171 | Loss : 6.2132391929626465\n",
      "Epoch : 172 | Loss : 5.82445764541626\n",
      "Epoch : 173 | Loss : 5.4587626457214355\n",
      "Epoch : 174 | Loss : 5.1154069900512695\n",
      "Epoch : 175 | Loss : 4.793652534484863\n",
      "Epoch : 176 | Loss : 4.492732524871826\n",
      "Epoch : 177 | Loss : 4.211877346038818\n",
      "Epoch : 178 | Loss : 3.9502711296081543\n",
      "Epoch : 179 | Loss : 3.7071352005004883\n",
      "Epoch : 180 | Loss : 3.481656074523926\n",
      "Epoch : 181 | Loss : 3.2730202674865723\n",
      "Epoch : 182 | Loss : 3.0804107189178467\n",
      "Epoch : 183 | Loss : 2.9030230045318604\n",
      "Epoch : 184 | Loss : 2.7400567531585693\n",
      "Epoch : 185 | Loss : 2.5907130241394043\n",
      "Epoch : 186 | Loss : 2.454206943511963\n",
      "Epoch : 187 | Loss : 2.3297665119171143\n",
      "Epoch : 188 | Loss : 2.216625690460205\n",
      "Epoch : 189 | Loss : 2.114053964614868\n",
      "Epoch : 190 | Loss : 2.021327257156372\n",
      "Epoch : 191 | Loss : 1.937757134437561\n",
      "Epoch : 192 | Loss : 1.8626599311828613\n",
      "Epoch : 193 | Loss : 1.7953821420669556\n",
      "Epoch : 194 | Loss : 1.7352992296218872\n",
      "Epoch : 195 | Loss : 1.681809663772583\n",
      "Epoch : 196 | Loss : 1.634348750114441\n",
      "Epoch : 197 | Loss : 1.5923718214035034\n",
      "Epoch : 198 | Loss : 1.5553734302520752\n",
      "Epoch : 199 | Loss : 1.5228675603866577\n",
      "Epoch : 200 | Loss : 1.4944052696228027\n",
      "Epoch : 201 | Loss : 1.4695611000061035\n",
      "Epoch : 202 | Loss : 1.4479427337646484\n",
      "Epoch : 203 | Loss : 1.4291889667510986\n",
      "Epoch : 204 | Loss : 1.412962794303894\n",
      "Epoch : 205 | Loss : 1.3989566564559937\n",
      "Epoch : 206 | Loss : 1.386885404586792\n",
      "Epoch : 207 | Loss : 1.376496434211731\n",
      "Epoch : 208 | Loss : 1.367558240890503\n",
      "Epoch : 209 | Loss : 1.3598593473434448\n",
      "Epoch : 210 | Loss : 1.3532118797302246\n",
      "Epoch : 211 | Loss : 1.347451090812683\n",
      "Epoch : 212 | Loss : 1.3424303531646729\n",
      "Epoch : 213 | Loss : 1.338017225265503\n",
      "Epoch : 214 | Loss : 1.334099292755127\n",
      "Epoch : 215 | Loss : 1.3305771350860596\n",
      "Epoch : 216 | Loss : 1.3273674249649048\n",
      "Epoch : 217 | Loss : 1.3243988752365112\n",
      "Epoch : 218 | Loss : 1.3216075897216797\n",
      "Epoch : 219 | Loss : 1.3189430236816406\n",
      "Epoch : 220 | Loss : 1.3163634538650513\n",
      "Epoch : 221 | Loss : 1.3138344287872314\n",
      "Epoch : 222 | Loss : 1.3113261461257935\n",
      "Epoch : 223 | Loss : 1.308822751045227\n",
      "Epoch : 224 | Loss : 1.3063026666641235\n",
      "Epoch : 225 | Loss : 1.3037561178207397\n",
      "Epoch : 226 | Loss : 1.301174283027649\n",
      "Epoch : 227 | Loss : 1.2985503673553467\n",
      "Epoch : 228 | Loss : 1.2958850860595703\n",
      "Epoch : 229 | Loss : 1.2931751012802124\n",
      "Epoch : 230 | Loss : 1.2904223203659058\n",
      "Epoch : 231 | Loss : 1.2876276969909668\n",
      "Epoch : 232 | Loss : 1.2847974300384521\n",
      "Epoch : 233 | Loss : 1.2819329500198364\n",
      "Epoch : 234 | Loss : 1.279037594795227\n",
      "Epoch : 235 | Loss : 1.2761176824569702\n",
      "Epoch : 236 | Loss : 1.2731783390045166\n",
      "Epoch : 237 | Loss : 1.2702209949493408\n",
      "Epoch : 238 | Loss : 1.2672531604766846\n",
      "Epoch : 239 | Loss : 1.2642756700515747\n",
      "Epoch : 240 | Loss : 1.2612953186035156\n",
      "Epoch : 241 | Loss : 1.2583136558532715\n",
      "Epoch : 242 | Loss : 1.255334734916687\n",
      "Epoch : 243 | Loss : 1.2523589134216309\n",
      "Epoch : 244 | Loss : 1.2493922710418701\n",
      "Epoch : 245 | Loss : 1.246433138847351\n",
      "Epoch : 246 | Loss : 1.2434862852096558\n",
      "Epoch : 247 | Loss : 1.2405509948730469\n",
      "Epoch : 248 | Loss : 1.2376292943954468\n",
      "Epoch : 249 | Loss : 1.23472261428833\n",
      "Epoch : 250 | Loss : 1.2318283319473267\n",
      "Epoch : 251 | Loss : 1.228948950767517\n",
      "Epoch : 252 | Loss : 1.2260850667953491\n",
      "Epoch : 253 | Loss : 1.2232359647750854\n",
      "Epoch : 254 | Loss : 1.2204082012176514\n",
      "Epoch : 255 | Loss : 1.2175955772399902\n",
      "Epoch : 256 | Loss : 1.2147960662841797\n",
      "Epoch : 257 | Loss : 1.2120109796524048\n",
      "Epoch : 258 | Loss : 1.2092392444610596\n",
      "Epoch : 259 | Loss : 1.206479787826538\n",
      "Epoch : 260 | Loss : 1.2037341594696045\n",
      "Epoch : 261 | Loss : 1.2009985446929932\n",
      "Epoch : 262 | Loss : 1.1982731819152832\n",
      "Epoch : 263 | Loss : 1.1955598592758179\n",
      "Epoch : 264 | Loss : 1.1928554773330688\n",
      "Epoch : 265 | Loss : 1.1901592016220093\n",
      "Epoch : 266 | Loss : 1.187474012374878\n",
      "Epoch : 267 | Loss : 1.1847946643829346\n",
      "Epoch : 268 | Loss : 1.1821235418319702\n",
      "Epoch : 269 | Loss : 1.1794599294662476\n",
      "Epoch : 270 | Loss : 1.1768032312393188\n",
      "Epoch : 271 | Loss : 1.1741526126861572\n",
      "Epoch : 272 | Loss : 1.1715099811553955\n",
      "Epoch : 273 | Loss : 1.168871283531189\n",
      "Epoch : 274 | Loss : 1.166237711906433\n",
      "Epoch : 275 | Loss : 1.163610816001892\n",
      "Epoch : 276 | Loss : 1.16098952293396\n",
      "Epoch : 277 | Loss : 1.1583714485168457\n",
      "Epoch : 278 | Loss : 1.1557600498199463\n",
      "Epoch : 279 | Loss : 1.1531519889831543\n",
      "Epoch : 280 | Loss : 1.150550365447998\n",
      "Epoch : 281 | Loss : 1.1479512453079224\n",
      "Epoch : 282 | Loss : 1.1453592777252197\n",
      "Epoch : 283 | Loss : 1.1427690982818604\n",
      "Epoch : 284 | Loss : 1.1401844024658203\n",
      "Epoch : 285 | Loss : 1.1376049518585205\n",
      "Epoch : 286 | Loss : 1.135027289390564\n",
      "Epoch : 287 | Loss : 1.1324572563171387\n",
      "Epoch : 288 | Loss : 1.129888892173767\n",
      "Epoch : 289 | Loss : 1.1273267269134521\n",
      "Epoch : 290 | Loss : 1.1247671842575073\n",
      "Epoch : 291 | Loss : 1.122213363647461\n",
      "Epoch : 292 | Loss : 1.1196621656417847\n",
      "Epoch : 293 | Loss : 1.1171174049377441\n",
      "Epoch : 294 | Loss : 1.114574670791626\n",
      "Epoch : 295 | Loss : 1.1120390892028809\n",
      "Epoch : 296 | Loss : 1.1095062494277954\n",
      "Epoch : 297 | Loss : 1.1069775819778442\n",
      "Epoch : 298 | Loss : 1.1044541597366333\n",
      "Epoch : 299 | Loss : 1.1019341945648193\n",
      "Epoch : 300 | Loss : 1.099419116973877\n",
      "Epoch : 301 | Loss : 1.0969101190567017\n",
      "Epoch : 302 | Loss : 1.0944024324417114\n",
      "Epoch : 303 | Loss : 1.091900110244751\n",
      "Epoch : 304 | Loss : 1.0894033908843994\n",
      "Epoch : 305 | Loss : 1.0869110822677612\n",
      "Epoch : 306 | Loss : 1.0844218730926514\n",
      "Epoch : 307 | Loss : 1.081938624382019\n",
      "Epoch : 308 | Loss : 1.0794589519500732\n",
      "Epoch : 309 | Loss : 1.076984167098999\n",
      "Epoch : 310 | Loss : 1.0745137929916382\n",
      "Epoch : 311 | Loss : 1.0720481872558594\n",
      "Epoch : 312 | Loss : 1.0695866346359253\n",
      "Epoch : 313 | Loss : 1.067130446434021\n",
      "Epoch : 314 | Loss : 1.0646774768829346\n",
      "Epoch : 315 | Loss : 1.0622280836105347\n",
      "Epoch : 316 | Loss : 1.0597845315933228\n",
      "Epoch : 317 | Loss : 1.0573461055755615\n",
      "Epoch : 318 | Loss : 1.0549100637435913\n",
      "Epoch : 319 | Loss : 1.0524811744689941\n",
      "Epoch : 320 | Loss : 1.0500538349151611\n",
      "Epoch : 321 | Loss : 1.0476332902908325\n",
      "Epoch : 322 | Loss : 1.0452167987823486\n",
      "Epoch : 323 | Loss : 1.0428038835525513\n",
      "Epoch : 324 | Loss : 1.0403952598571777\n",
      "Epoch : 325 | Loss : 1.0379912853240967\n",
      "Epoch : 326 | Loss : 1.0355925559997559\n",
      "Epoch : 327 | Loss : 1.0331971645355225\n",
      "Epoch : 328 | Loss : 1.0308072566986084\n",
      "Epoch : 329 | Loss : 1.0284221172332764\n",
      "Epoch : 330 | Loss : 1.0260405540466309\n",
      "Epoch : 331 | Loss : 1.0236625671386719\n",
      "Epoch : 332 | Loss : 1.0212891101837158\n",
      "Epoch : 333 | Loss : 1.018922209739685\n",
      "Epoch : 334 | Loss : 1.0165585279464722\n",
      "Epoch : 335 | Loss : 1.0141985416412354\n",
      "Epoch : 336 | Loss : 1.0118440389633179\n",
      "Epoch : 337 | Loss : 1.0094927549362183\n",
      "Epoch : 338 | Loss : 1.0071479082107544\n",
      "Epoch : 339 | Loss : 1.0048043727874756\n",
      "Epoch : 340 | Loss : 1.0024679899215698\n",
      "Epoch : 341 | Loss : 1.0001354217529297\n",
      "Epoch : 342 | Loss : 0.9978057146072388\n",
      "Epoch : 343 | Loss : 0.9954825043678284\n",
      "Epoch : 344 | Loss : 0.9931628704071045\n",
      "Epoch : 345 | Loss : 0.9908463954925537\n",
      "Epoch : 346 | Loss : 0.9885360598564148\n",
      "Epoch : 347 | Loss : 0.9862292408943176\n",
      "Epoch : 348 | Loss : 0.9839264154434204\n",
      "Epoch : 349 | Loss : 0.9816286563873291\n",
      "Epoch : 350 | Loss : 0.9793359637260437\n",
      "Epoch : 351 | Loss : 0.977046549320221\n",
      "Epoch : 352 | Loss : 0.9747623205184937\n",
      "Epoch : 353 | Loss : 0.9724807739257812\n",
      "Epoch : 354 | Loss : 0.9702047109603882\n",
      "Epoch : 355 | Loss : 0.9679338335990906\n",
      "Epoch : 356 | Loss : 0.9656649827957153\n",
      "Epoch : 357 | Loss : 0.9634014964103699\n",
      "Epoch : 358 | Loss : 0.9611437320709229\n",
      "Epoch : 359 | Loss : 0.9588897824287415\n",
      "Epoch : 360 | Loss : 0.9566386342048645\n",
      "Epoch : 361 | Loss : 0.9543932676315308\n",
      "Epoch : 362 | Loss : 0.9521524310112\n",
      "Epoch : 363 | Loss : 0.9499154686927795\n",
      "Epoch : 364 | Loss : 0.9476833343505859\n",
      "Epoch : 365 | Loss : 0.9454532265663147\n",
      "Epoch : 366 | Loss : 0.9432291388511658\n",
      "Epoch : 367 | Loss : 0.9410101771354675\n",
      "Epoch : 368 | Loss : 0.9387949705123901\n",
      "Epoch : 369 | Loss : 0.9365832209587097\n",
      "Epoch : 370 | Loss : 0.9343757629394531\n",
      "Epoch : 371 | Loss : 0.9321731328964233\n",
      "Epoch : 372 | Loss : 0.9299739003181458\n",
      "Epoch : 373 | Loss : 0.9277795553207397\n",
      "Epoch : 374 | Loss : 0.9255895018577576\n",
      "Epoch : 375 | Loss : 0.9234047532081604\n",
      "Epoch : 376 | Loss : 0.9212229251861572\n",
      "Epoch : 377 | Loss : 0.9190459847450256\n",
      "Epoch : 378 | Loss : 0.9168726801872253\n",
      "Epoch : 379 | Loss : 0.9147046208381653\n",
      "Epoch : 380 | Loss : 0.9125400185585022\n",
      "Epoch : 381 | Loss : 0.9103794097900391\n",
      "Epoch : 382 | Loss : 0.9082232117652893\n",
      "Epoch : 383 | Loss : 0.9060726761817932\n",
      "Epoch : 384 | Loss : 0.9039245843887329\n",
      "Epoch : 385 | Loss : 0.9017809629440308\n",
      "Epoch : 386 | Loss : 0.8996409773826599\n",
      "Epoch : 387 | Loss : 0.8975083231925964\n",
      "Epoch : 388 | Loss : 0.8953768014907837\n",
      "Epoch : 389 | Loss : 0.8932503461837769\n",
      "Epoch : 390 | Loss : 0.8911287188529968\n",
      "Epoch : 391 | Loss : 0.8890102505683899\n",
      "Epoch : 392 | Loss : 0.8868958950042725\n",
      "Epoch : 393 | Loss : 0.8847867250442505\n",
      "Epoch : 394 | Loss : 0.882680356502533\n",
      "Epoch : 395 | Loss : 0.8805792927742004\n",
      "Epoch : 396 | Loss : 0.8784828186035156\n",
      "Epoch : 397 | Loss : 0.8763893842697144\n",
      "Epoch : 398 | Loss : 0.8743001222610474\n",
      "Epoch : 399 | Loss : 0.8722159266471863\n",
      "Epoch : 400 | Loss : 0.8701364994049072\n",
      "Epoch : 401 | Loss : 0.8680590987205505\n",
      "Epoch : 402 | Loss : 0.865986704826355\n",
      "Epoch : 403 | Loss : 0.8639185428619385\n",
      "Epoch : 404 | Loss : 0.8618545532226562\n",
      "Epoch : 405 | Loss : 0.8597950339317322\n",
      "Epoch : 406 | Loss : 0.8577390313148499\n",
      "Epoch : 407 | Loss : 0.8556875586509705\n",
      "Epoch : 408 | Loss : 0.8536393046379089\n",
      "Epoch : 409 | Loss : 0.8515952825546265\n",
      "Epoch : 410 | Loss : 0.8495554327964783\n",
      "Epoch : 411 | Loss : 0.8475212454795837\n",
      "Epoch : 412 | Loss : 0.8454886674880981\n",
      "Epoch : 413 | Loss : 0.8434619903564453\n",
      "Epoch : 414 | Loss : 0.8414390087127686\n",
      "Epoch : 415 | Loss : 0.8394189476966858\n",
      "Epoch : 416 | Loss : 0.8374053835868835\n",
      "Epoch : 417 | Loss : 0.8353930711746216\n",
      "Epoch : 418 | Loss : 0.8333866596221924\n",
      "Epoch : 419 | Loss : 0.8313844203948975\n",
      "Epoch : 420 | Loss : 0.8293848037719727\n",
      "Epoch : 421 | Loss : 0.8273897767066956\n",
      "Epoch : 422 | Loss : 0.8253993391990662\n",
      "Epoch : 423 | Loss : 0.8234110474586487\n",
      "Epoch : 424 | Loss : 0.8214282989501953\n",
      "Epoch : 425 | Loss : 0.8194499611854553\n",
      "Epoch : 426 | Loss : 0.8174750208854675\n",
      "Epoch : 427 | Loss : 0.8155044317245483\n",
      "Epoch : 428 | Loss : 0.8135372996330261\n",
      "Epoch : 429 | Loss : 0.8115736842155457\n",
      "Epoch : 430 | Loss : 0.8096157312393188\n",
      "Epoch : 431 | Loss : 0.8076606392860413\n",
      "Epoch : 432 | Loss : 0.8057093024253845\n",
      "Epoch : 433 | Loss : 0.8037616610527039\n",
      "Epoch : 434 | Loss : 0.8018192648887634\n",
      "Epoch : 435 | Loss : 0.7998791337013245\n",
      "Epoch : 436 | Loss : 0.7979438304901123\n",
      "Epoch : 437 | Loss : 0.7960118055343628\n",
      "Epoch : 438 | Loss : 0.7940851449966431\n",
      "Epoch : 439 | Loss : 0.7921613454818726\n",
      "Epoch : 440 | Loss : 0.7902414798736572\n",
      "Epoch : 441 | Loss : 0.7883266806602478\n",
      "Epoch : 442 | Loss : 0.786413848400116\n",
      "Epoch : 443 | Loss : 0.7845069169998169\n",
      "Epoch : 444 | Loss : 0.7826018929481506\n",
      "Epoch : 445 | Loss : 0.7807028293609619\n",
      "Epoch : 446 | Loss : 0.7788061499595642\n",
      "Epoch : 447 | Loss : 0.7769138216972351\n",
      "Epoch : 448 | Loss : 0.7750245332717896\n",
      "Epoch : 449 | Loss : 0.7731401324272156\n",
      "Epoch : 450 | Loss : 0.7712590098381042\n",
      "Epoch : 451 | Loss : 0.7693824768066406\n",
      "Epoch : 452 | Loss : 0.7675084471702576\n",
      "Epoch : 453 | Loss : 0.7656402587890625\n",
      "Epoch : 454 | Loss : 0.763775110244751\n",
      "Epoch : 455 | Loss : 0.7619119882583618\n",
      "Epoch : 456 | Loss : 0.7600546479225159\n",
      "Epoch : 457 | Loss : 0.7582003474235535\n",
      "Epoch : 458 | Loss : 0.7563494443893433\n",
      "Epoch : 459 | Loss : 0.7545034289360046\n",
      "Epoch : 460 | Loss : 0.7526598572731018\n",
      "Epoch : 461 | Loss : 0.7508215308189392\n",
      "Epoch : 462 | Loss : 0.7489868402481079\n",
      "Epoch : 463 | Loss : 0.7471553683280945\n",
      "Epoch : 464 | Loss : 0.7453275322914124\n",
      "Epoch : 465 | Loss : 0.7435033321380615\n",
      "Epoch : 466 | Loss : 0.7416844964027405\n",
      "Epoch : 467 | Loss : 0.7398681044578552\n",
      "Epoch : 468 | Loss : 0.7380550503730774\n",
      "Epoch : 469 | Loss : 0.7362456917762756\n",
      "Epoch : 470 | Loss : 0.7344394326210022\n",
      "Epoch : 471 | Loss : 0.7326385378837585\n",
      "Epoch : 472 | Loss : 0.7308412194252014\n",
      "Epoch : 473 | Loss : 0.7290474772453308\n",
      "Epoch : 474 | Loss : 0.7272558808326721\n",
      "Epoch : 475 | Loss : 0.7254695892333984\n",
      "Epoch : 476 | Loss : 0.7236870527267456\n",
      "Epoch : 477 | Loss : 0.721908450126648\n",
      "Epoch : 478 | Loss : 0.7201326489448547\n",
      "Epoch : 479 | Loss : 0.7183601260185242\n",
      "Epoch : 480 | Loss : 0.7165914177894592\n",
      "Epoch : 481 | Loss : 0.7148271203041077\n",
      "Epoch : 482 | Loss : 0.7130661010742188\n",
      "Epoch : 483 | Loss : 0.7113085985183716\n",
      "Epoch : 484 | Loss : 0.709555447101593\n",
      "Epoch : 485 | Loss : 0.7078055739402771\n",
      "Epoch : 486 | Loss : 0.7060582637786865\n",
      "Epoch : 487 | Loss : 0.7043155431747437\n",
      "Epoch : 488 | Loss : 0.7025762796401978\n",
      "Epoch : 489 | Loss : 0.7008407711982727\n",
      "Epoch : 490 | Loss : 0.699108362197876\n",
      "Epoch : 491 | Loss : 0.6973804235458374\n",
      "Epoch : 492 | Loss : 0.6956552863121033\n",
      "Epoch : 493 | Loss : 0.6939342021942139\n",
      "Epoch : 494 | Loss : 0.6922167539596558\n",
      "Epoch : 495 | Loss : 0.6905028820037842\n",
      "Epoch : 496 | Loss : 0.6887915134429932\n",
      "Epoch : 497 | Loss : 0.6870844960212708\n",
      "Epoch : 498 | Loss : 0.6853813529014587\n",
      "Epoch : 499 | Loss : 0.6836850643157959\n",
      "Epoch : 500 | Loss : 0.6819919943809509\n",
      "Epoch : 501 | Loss : 0.6803017258644104\n",
      "Epoch : 502 | Loss : 0.6786167025566101\n",
      "Epoch : 503 | Loss : 0.6769342422485352\n",
      "Epoch : 504 | Loss : 0.6752567291259766\n",
      "Epoch : 505 | Loss : 0.6735817193984985\n",
      "Epoch : 506 | Loss : 0.6719115376472473\n",
      "Epoch : 507 | Loss : 0.6702428460121155\n",
      "Epoch : 508 | Loss : 0.6685788631439209\n",
      "Epoch : 509 | Loss : 0.6669188737869263\n",
      "Epoch : 510 | Loss : 0.6652611494064331\n",
      "Epoch : 511 | Loss : 0.6636078357696533\n",
      "Epoch : 512 | Loss : 0.6619585156440735\n",
      "Epoch : 513 | Loss : 0.6603111624717712\n",
      "Epoch : 514 | Loss : 0.6586689352989197\n",
      "Epoch : 515 | Loss : 0.6570296883583069\n",
      "Epoch : 516 | Loss : 0.6553940176963806\n",
      "Epoch : 517 | Loss : 0.6537601947784424\n",
      "Epoch : 518 | Loss : 0.6521325707435608\n",
      "Epoch : 519 | Loss : 0.6505054235458374\n",
      "Epoch : 520 | Loss : 0.6488834619522095\n",
      "Epoch : 521 | Loss : 0.6472647190093994\n",
      "Epoch : 522 | Loss : 0.6456489562988281\n",
      "Epoch : 523 | Loss : 0.6440368890762329\n",
      "Epoch : 524 | Loss : 0.6424278616905212\n",
      "Epoch : 525 | Loss : 0.6408230066299438\n",
      "Epoch : 526 | Loss : 0.6392204165458679\n",
      "Epoch : 527 | Loss : 0.6376230716705322\n",
      "Epoch : 528 | Loss : 0.6360272765159607\n",
      "Epoch : 529 | Loss : 0.6344358921051025\n",
      "Epoch : 530 | Loss : 0.6328468322753906\n",
      "Epoch : 531 | Loss : 0.6312621831893921\n",
      "Epoch : 532 | Loss : 0.6296800971031189\n",
      "Epoch : 533 | Loss : 0.6281017661094666\n",
      "Epoch : 534 | Loss : 0.6265259981155396\n",
      "Epoch : 535 | Loss : 0.6249549984931946\n",
      "Epoch : 536 | Loss : 0.6233857870101929\n",
      "Epoch : 537 | Loss : 0.6218218207359314\n",
      "Epoch : 538 | Loss : 0.6202586889266968\n",
      "Epoch : 539 | Loss : 0.6187008619308472\n",
      "Epoch : 540 | Loss : 0.6171451807022095\n",
      "Epoch : 541 | Loss : 0.6155925989151001\n",
      "Epoch : 542 | Loss : 0.6140437722206116\n",
      "Epoch : 543 | Loss : 0.6124981641769409\n",
      "Epoch : 544 | Loss : 0.6109570860862732\n",
      "Epoch : 545 | Loss : 0.6094171404838562\n",
      "Epoch : 546 | Loss : 0.6078813672065735\n",
      "Epoch : 547 | Loss : 0.6063488721847534\n",
      "Epoch : 548 | Loss : 0.6048192977905273\n",
      "Epoch : 549 | Loss : 0.6032942533493042\n",
      "Epoch : 550 | Loss : 0.6017705798149109\n",
      "Epoch : 551 | Loss : 0.6002511978149414\n",
      "Epoch : 552 | Loss : 0.5987347960472107\n",
      "Epoch : 553 | Loss : 0.5972210764884949\n",
      "Epoch : 554 | Loss : 0.5957111716270447\n",
      "Epoch : 555 | Loss : 0.5942046046257019\n",
      "Epoch : 556 | Loss : 0.5927009582519531\n",
      "Epoch : 557 | Loss : 0.5912001132965088\n",
      "Epoch : 558 | Loss : 0.5897029638290405\n",
      "Epoch : 559 | Loss : 0.588209331035614\n",
      "Epoch : 560 | Loss : 0.5867183208465576\n",
      "Epoch : 561 | Loss : 0.585229754447937\n",
      "Epoch : 562 | Loss : 0.583745539188385\n",
      "Epoch : 563 | Loss : 0.5822646617889404\n",
      "Epoch : 564 | Loss : 0.5807861089706421\n",
      "Epoch : 565 | Loss : 0.5793102979660034\n",
      "Epoch : 566 | Loss : 0.5778383612632751\n",
      "Epoch : 567 | Loss : 0.5763698816299438\n",
      "Epoch : 568 | Loss : 0.5749043822288513\n",
      "Epoch : 569 | Loss : 0.5734409689903259\n",
      "Epoch : 570 | Loss : 0.5719817280769348\n",
      "Epoch : 571 | Loss : 0.5705246329307556\n",
      "Epoch : 572 | Loss : 0.5690711736679077\n",
      "Epoch : 573 | Loss : 0.5676208734512329\n",
      "Epoch : 574 | Loss : 0.566173791885376\n",
      "Epoch : 575 | Loss : 0.564729630947113\n",
      "Epoch : 576 | Loss : 0.5632883906364441\n",
      "Epoch : 577 | Loss : 0.5618499517440796\n",
      "Epoch : 578 | Loss : 0.560416042804718\n",
      "Epoch : 579 | Loss : 0.5589833855628967\n",
      "Epoch : 580 | Loss : 0.5575544834136963\n",
      "Epoch : 581 | Loss : 0.5561278462409973\n",
      "Epoch : 582 | Loss : 0.5547052025794983\n",
      "Epoch : 583 | Loss : 0.5532853603363037\n",
      "Epoch : 584 | Loss : 0.5518699884414673\n",
      "Epoch : 585 | Loss : 0.5504553914070129\n",
      "Epoch : 586 | Loss : 0.5490443110466003\n",
      "Epoch : 587 | Loss : 0.5476363897323608\n",
      "Epoch : 588 | Loss : 0.5462318062782288\n",
      "Epoch : 589 | Loss : 0.5448304414749146\n",
      "Epoch : 590 | Loss : 0.543431282043457\n",
      "Epoch : 591 | Loss : 0.542035698890686\n",
      "Epoch : 592 | Loss : 0.540643036365509\n",
      "Epoch : 593 | Loss : 0.5392524003982544\n",
      "Epoch : 594 | Loss : 0.5378659963607788\n",
      "Epoch : 595 | Loss : 0.5364820957183838\n",
      "Epoch : 596 | Loss : 0.5351022481918335\n",
      "Epoch : 597 | Loss : 0.5337238907814026\n",
      "Epoch : 598 | Loss : 0.5323486924171448\n",
      "Epoch : 599 | Loss : 0.5309770703315735\n",
      "Epoch : 600 | Loss : 0.5296081304550171\n",
      "Epoch : 601 | Loss : 0.5282413959503174\n",
      "Epoch : 602 | Loss : 0.5268763899803162\n",
      "Epoch : 603 | Loss : 0.5255175828933716\n",
      "Epoch : 604 | Loss : 0.52415931224823\n",
      "Epoch : 605 | Loss : 0.5228055715560913\n",
      "Epoch : 606 | Loss : 0.5214534997940063\n",
      "Epoch : 607 | Loss : 0.5201045870780945\n",
      "Epoch : 608 | Loss : 0.5187589526176453\n",
      "Epoch : 609 | Loss : 0.5174154043197632\n",
      "Epoch : 610 | Loss : 0.5160759091377258\n",
      "Epoch : 611 | Loss : 0.5147382616996765\n",
      "Epoch : 612 | Loss : 0.5134041905403137\n",
      "Epoch : 613 | Loss : 0.5120725631713867\n",
      "Epoch : 614 | Loss : 0.5107434988021851\n",
      "Epoch : 615 | Loss : 0.5094183683395386\n",
      "Epoch : 616 | Loss : 0.5080957412719727\n",
      "Epoch : 617 | Loss : 0.5067744255065918\n",
      "Epoch : 618 | Loss : 0.5054578185081482\n",
      "Epoch : 619 | Loss : 0.5041437149047852\n",
      "Epoch : 620 | Loss : 0.502831220626831\n",
      "Epoch : 621 | Loss : 0.5015231370925903\n",
      "Epoch : 622 | Loss : 0.5002167820930481\n",
      "Epoch : 623 | Loss : 0.498913437128067\n",
      "Epoch : 624 | Loss : 0.4976128935813904\n",
      "Epoch : 625 | Loss : 0.4963151514530182\n",
      "Epoch : 626 | Loss : 0.49502089619636536\n",
      "Epoch : 627 | Loss : 0.4937289357185364\n",
      "Epoch : 628 | Loss : 0.49243947863578796\n",
      "Epoch : 629 | Loss : 0.49115294218063354\n",
      "Epoch : 630 | Loss : 0.48986920714378357\n",
      "Epoch : 631 | Loss : 0.48858872056007385\n",
      "Epoch : 632 | Loss : 0.4873109757900238\n",
      "Epoch : 633 | Loss : 0.48603495955467224\n",
      "Epoch : 634 | Loss : 0.4847622215747833\n",
      "Epoch : 635 | Loss : 0.48349228501319885\n",
      "Epoch : 636 | Loss : 0.48222559690475464\n",
      "Epoch : 637 | Loss : 0.48096081614494324\n",
      "Epoch : 638 | Loss : 0.4796992540359497\n",
      "Epoch : 639 | Loss : 0.47843995690345764\n",
      "Epoch : 640 | Loss : 0.4771842658519745\n",
      "Epoch : 641 | Loss : 0.4759312868118286\n",
      "Epoch : 642 | Loss : 0.4746798276901245\n",
      "Epoch : 643 | Loss : 0.4734320044517517\n",
      "Epoch : 644 | Loss : 0.47218650579452515\n",
      "Epoch : 645 | Loss : 0.4709438979625702\n",
      "Epoch : 646 | Loss : 0.4697044789791107\n",
      "Epoch : 647 | Loss : 0.4684673845767975\n",
      "Epoch : 648 | Loss : 0.46723222732543945\n",
      "Epoch : 649 | Loss : 0.46599939465522766\n",
      "Epoch : 650 | Loss : 0.4647713303565979\n",
      "Epoch : 651 | Loss : 0.463544100522995\n",
      "Epoch : 652 | Loss : 0.46232038736343384\n",
      "Epoch : 653 | Loss : 0.46109849214553833\n",
      "Epoch : 654 | Loss : 0.45987996459007263\n",
      "Epoch : 655 | Loss : 0.4586638808250427\n",
      "Epoch : 656 | Loss : 0.45745065808296204\n",
      "Epoch : 657 | Loss : 0.4562405049800873\n",
      "Epoch : 658 | Loss : 0.45503151416778564\n",
      "Epoch : 659 | Loss : 0.45382651686668396\n",
      "Epoch : 660 | Loss : 0.4526230990886688\n",
      "Epoch : 661 | Loss : 0.45142343640327454\n",
      "Epoch : 662 | Loss : 0.45022597908973694\n",
      "Epoch : 663 | Loss : 0.44903072714805603\n",
      "Epoch : 664 | Loss : 0.4478383958339691\n",
      "Epoch : 665 | Loss : 0.44664838910102844\n",
      "Epoch : 666 | Loss : 0.4454613924026489\n",
      "Epoch : 667 | Loss : 0.4442765414714813\n",
      "Epoch : 668 | Loss : 0.4430941641330719\n",
      "Epoch : 669 | Loss : 0.4419156312942505\n",
      "Epoch : 670 | Loss : 0.4407379925251007\n",
      "Epoch : 671 | Loss : 0.4395633637905121\n",
      "Epoch : 672 | Loss : 0.43839189410209656\n",
      "Epoch : 673 | Loss : 0.43722331523895264\n",
      "Epoch : 674 | Loss : 0.4360559582710266\n",
      "Epoch : 675 | Loss : 0.4348912537097931\n",
      "Epoch : 676 | Loss : 0.4337303042411804\n",
      "Epoch : 677 | Loss : 0.43257102370262146\n",
      "Epoch : 678 | Loss : 0.43141427636146545\n",
      "Epoch : 679 | Loss : 0.4302601218223572\n",
      "Epoch : 680 | Loss : 0.4291086792945862\n",
      "Epoch : 681 | Loss : 0.42795947194099426\n",
      "Epoch : 682 | Loss : 0.4268133342266083\n",
      "Epoch : 683 | Loss : 0.4256695508956909\n",
      "Epoch : 684 | Loss : 0.4245280623435974\n",
      "Epoch : 685 | Loss : 0.4233890175819397\n",
      "Epoch : 686 | Loss : 0.4222525954246521\n",
      "Epoch : 687 | Loss : 0.4211191236972809\n",
      "Epoch : 688 | Loss : 0.419986754655838\n",
      "Epoch : 689 | Loss : 0.4188583195209503\n",
      "Epoch : 690 | Loss : 0.4177321493625641\n",
      "Epoch : 691 | Loss : 0.41660717129707336\n",
      "Epoch : 692 | Loss : 0.4154856503009796\n",
      "Epoch : 693 | Loss : 0.4143672287464142\n",
      "Epoch : 694 | Loss : 0.4132498800754547\n",
      "Epoch : 695 | Loss : 0.41213616728782654\n",
      "Epoch : 696 | Loss : 0.41102442145347595\n",
      "Epoch : 697 | Loss : 0.4099154770374298\n",
      "Epoch : 698 | Loss : 0.4088079035282135\n",
      "Epoch : 699 | Loss : 0.40770360827445984\n",
      "Epoch : 700 | Loss : 0.4066016972064972\n",
      "Epoch : 701 | Loss : 0.40550151467323303\n",
      "Epoch : 702 | Loss : 0.40440458059310913\n",
      "Epoch : 703 | Loss : 0.4033104181289673\n",
      "Epoch : 704 | Loss : 0.40221768617630005\n",
      "Epoch : 705 | Loss : 0.40112778544425964\n",
      "Epoch : 706 | Loss : 0.40003976225852966\n",
      "Epoch : 707 | Loss : 0.39895543456077576\n",
      "Epoch : 708 | Loss : 0.3978719711303711\n",
      "Epoch : 709 | Loss : 0.39679205417633057\n",
      "Epoch : 710 | Loss : 0.39571449160575867\n",
      "Epoch : 711 | Loss : 0.3946382701396942\n",
      "Epoch : 712 | Loss : 0.3935648798942566\n",
      "Epoch : 713 | Loss : 0.392494261264801\n",
      "Epoch : 714 | Loss : 0.391425758600235\n",
      "Epoch : 715 | Loss : 0.3903590738773346\n",
      "Epoch : 716 | Loss : 0.38929542899131775\n",
      "Epoch : 717 | Loss : 0.38823455572128296\n",
      "Epoch : 718 | Loss : 0.3871745467185974\n",
      "Epoch : 719 | Loss : 0.3861185908317566\n",
      "Epoch : 720 | Loss : 0.385064035654068\n",
      "Epoch : 721 | Loss : 0.3840121626853943\n",
      "Epoch : 722 | Loss : 0.38296258449554443\n",
      "Epoch : 723 | Loss : 0.3819149434566498\n",
      "Epoch : 724 | Loss : 0.38087019324302673\n",
      "Epoch : 725 | Loss : 0.37982606887817383\n",
      "Epoch : 726 | Loss : 0.3787858188152313\n",
      "Epoch : 727 | Loss : 0.37774813175201416\n",
      "Epoch : 728 | Loss : 0.37671253085136414\n",
      "Epoch : 729 | Loss : 0.37567901611328125\n",
      "Epoch : 730 | Loss : 0.37464797496795654\n",
      "Epoch : 731 | Loss : 0.373618483543396\n",
      "Epoch : 732 | Loss : 0.3725917935371399\n",
      "Epoch : 733 | Loss : 0.3715673089027405\n",
      "Epoch : 734 | Loss : 0.3705454468727112\n",
      "Epoch : 735 | Loss : 0.36952492594718933\n",
      "Epoch : 736 | Loss : 0.3685080409049988\n",
      "Epoch : 737 | Loss : 0.3674927353858948\n",
      "Epoch : 738 | Loss : 0.3664790689945221\n",
      "Epoch : 739 | Loss : 0.36546847224235535\n",
      "Epoch : 740 | Loss : 0.36446017026901245\n",
      "Epoch : 741 | Loss : 0.3634543716907501\n",
      "Epoch : 742 | Loss : 0.36244991421699524\n",
      "Epoch : 743 | Loss : 0.36144793033599854\n",
      "Epoch : 744 | Loss : 0.36044901609420776\n",
      "Epoch : 745 | Loss : 0.3594512343406677\n",
      "Epoch : 746 | Loss : 0.35845574736595154\n",
      "Epoch : 747 | Loss : 0.3574635088443756\n",
      "Epoch : 748 | Loss : 0.3564724624156952\n",
      "Epoch : 749 | Loss : 0.3554845154285431\n",
      "Epoch : 750 | Loss : 0.35449734330177307\n",
      "Epoch : 751 | Loss : 0.35351383686065674\n",
      "Epoch : 752 | Loss : 0.3525315821170807\n",
      "Epoch : 753 | Loss : 0.3515520691871643\n",
      "Epoch : 754 | Loss : 0.35057464241981506\n",
      "Epoch : 755 | Loss : 0.3495997488498688\n",
      "Epoch : 756 | Loss : 0.34862667322158813\n",
      "Epoch : 757 | Loss : 0.3476555645465851\n",
      "Epoch : 758 | Loss : 0.34668663144111633\n",
      "Epoch : 759 | Loss : 0.34572041034698486\n",
      "Epoch : 760 | Loss : 0.34475573897361755\n",
      "Epoch : 761 | Loss : 0.3437940180301666\n",
      "Epoch : 762 | Loss : 0.3428339958190918\n",
      "Epoch : 763 | Loss : 0.3418758809566498\n",
      "Epoch : 764 | Loss : 0.34091997146606445\n",
      "Epoch : 765 | Loss : 0.3399662673473358\n",
      "Epoch : 766 | Loss : 0.33901500701904297\n",
      "Epoch : 767 | Loss : 0.3380658030509949\n",
      "Epoch : 768 | Loss : 0.3371186852455139\n",
      "Epoch : 769 | Loss : 0.3361734449863434\n",
      "Epoch : 770 | Loss : 0.33523017168045044\n",
      "Epoch : 771 | Loss : 0.3342891335487366\n",
      "Epoch : 772 | Loss : 0.3333507478237152\n",
      "Epoch : 773 | Loss : 0.33241501450538635\n",
      "Epoch : 774 | Loss : 0.3314800262451172\n",
      "Epoch : 775 | Loss : 0.33054739236831665\n",
      "Epoch : 776 | Loss : 0.3296170234680176\n",
      "Epoch : 777 | Loss : 0.3286893963813782\n",
      "Epoch : 778 | Loss : 0.3277633488178253\n",
      "Epoch : 779 | Loss : 0.32683971524238586\n",
      "Epoch : 780 | Loss : 0.32591739296913147\n",
      "Epoch : 781 | Loss : 0.32499805092811584\n",
      "Epoch : 782 | Loss : 0.32408013939857483\n",
      "Epoch : 783 | Loss : 0.32316476106643677\n",
      "Epoch : 784 | Loss : 0.3222511112689972\n",
      "Epoch : 785 | Loss : 0.3213396668434143\n",
      "Epoch : 786 | Loss : 0.320430189371109\n",
      "Epoch : 787 | Loss : 0.31952324509620667\n",
      "Epoch : 788 | Loss : 0.3186175525188446\n",
      "Epoch : 789 | Loss : 0.31771472096443176\n",
      "Epoch : 790 | Loss : 0.3168129324913025\n",
      "Epoch : 791 | Loss : 0.3159143030643463\n",
      "Epoch : 792 | Loss : 0.31501704454421997\n",
      "Epoch : 793 | Loss : 0.3141222596168518\n",
      "Epoch : 794 | Loss : 0.313229501247406\n",
      "Epoch : 795 | Loss : 0.3123382329940796\n",
      "Epoch : 796 | Loss : 0.3114488422870636\n",
      "Epoch : 797 | Loss : 0.31056201457977295\n",
      "Epoch : 798 | Loss : 0.30967703461647034\n",
      "Epoch : 799 | Loss : 0.3087945878505707\n",
      "Epoch : 800 | Loss : 0.30791303515434265\n",
      "Epoch : 801 | Loss : 0.3070344924926758\n",
      "Epoch : 802 | Loss : 0.3061577379703522\n",
      "Epoch : 803 | Loss : 0.3052825629711151\n",
      "Epoch : 804 | Loss : 0.304409921169281\n",
      "Epoch : 805 | Loss : 0.3035394251346588\n",
      "Epoch : 806 | Loss : 0.3026711046695709\n",
      "Epoch : 807 | Loss : 0.30180367827415466\n",
      "Epoch : 808 | Loss : 0.30093878507614136\n",
      "Epoch : 809 | Loss : 0.3000759780406952\n",
      "Epoch : 810 | Loss : 0.2992149591445923\n",
      "Epoch : 811 | Loss : 0.29835566878318787\n",
      "Epoch : 812 | Loss : 0.2974992096424103\n",
      "Epoch : 813 | Loss : 0.2966438829898834\n",
      "Epoch : 814 | Loss : 0.2957907021045685\n",
      "Epoch : 815 | Loss : 0.2949393689632416\n",
      "Epoch : 816 | Loss : 0.2940908670425415\n",
      "Epoch : 817 | Loss : 0.2932436764240265\n",
      "Epoch : 818 | Loss : 0.29239827394485474\n",
      "Epoch : 819 | Loss : 0.2915547490119934\n",
      "Epoch : 820 | Loss : 0.2907133102416992\n",
      "Epoch : 821 | Loss : 0.289874404668808\n",
      "Epoch : 822 | Loss : 0.28903692960739136\n",
      "Epoch : 823 | Loss : 0.28820115327835083\n",
      "Epoch : 824 | Loss : 0.28736746311187744\n",
      "Epoch : 825 | Loss : 0.2865361273288727\n",
      "Epoch : 826 | Loss : 0.28570643067359924\n",
      "Epoch : 827 | Loss : 0.2848779857158661\n",
      "Epoch : 828 | Loss : 0.28405195474624634\n",
      "Epoch : 829 | Loss : 0.2832283079624176\n",
      "Epoch : 830 | Loss : 0.2824055254459381\n",
      "Epoch : 831 | Loss : 0.2815857231616974\n",
      "Epoch : 832 | Loss : 0.2807669937610626\n",
      "Epoch : 833 | Loss : 0.2799505889415741\n",
      "Epoch : 834 | Loss : 0.2791363000869751\n",
      "Epoch : 835 | Loss : 0.2783235013484955\n",
      "Epoch : 836 | Loss : 0.2775129973888397\n",
      "Epoch : 837 | Loss : 0.2767033576965332\n",
      "Epoch : 838 | Loss : 0.27589693665504456\n",
      "Epoch : 839 | Loss : 0.27509135007858276\n",
      "Epoch : 840 | Loss : 0.2742882966995239\n",
      "Epoch : 841 | Loss : 0.27348658442497253\n",
      "Epoch : 842 | Loss : 0.27268731594085693\n",
      "Epoch : 843 | Loss : 0.27188870310783386\n",
      "Epoch : 844 | Loss : 0.27109354734420776\n",
      "Epoch : 845 | Loss : 0.2702990770339966\n",
      "Epoch : 846 | Loss : 0.26950758695602417\n",
      "Epoch : 847 | Loss : 0.2687171995639801\n",
      "Epoch : 848 | Loss : 0.26792821288108826\n",
      "Epoch : 849 | Loss : 0.2671425938606262\n",
      "Epoch : 850 | Loss : 0.2663569450378418\n",
      "Epoch : 851 | Loss : 0.26557397842407227\n",
      "Epoch : 852 | Loss : 0.2647932767868042\n",
      "Epoch : 853 | Loss : 0.2640134394168854\n",
      "Epoch : 854 | Loss : 0.263235867023468\n",
      "Epoch : 855 | Loss : 0.26246005296707153\n",
      "Epoch : 856 | Loss : 0.26168695092201233\n",
      "Epoch : 857 | Loss : 0.2609141767024994\n",
      "Epoch : 858 | Loss : 0.2601439356803894\n",
      "Epoch : 859 | Loss : 0.2593758702278137\n",
      "Epoch : 860 | Loss : 0.25860899686813354\n",
      "Epoch : 861 | Loss : 0.2578442394733429\n",
      "Epoch : 862 | Loss : 0.2570812404155731\n",
      "Epoch : 863 | Loss : 0.2563198208808899\n",
      "Epoch : 864 | Loss : 0.2555602192878723\n",
      "Epoch : 865 | Loss : 0.2548028826713562\n",
      "Epoch : 866 | Loss : 0.25404658913612366\n",
      "Epoch : 867 | Loss : 0.25329282879829407\n",
      "Epoch : 868 | Loss : 0.2525399625301361\n",
      "Epoch : 869 | Loss : 0.2517894208431244\n",
      "Epoch : 870 | Loss : 0.25104063749313354\n",
      "Epoch : 871 | Loss : 0.25029388070106506\n",
      "Epoch : 872 | Loss : 0.24954864382743835\n",
      "Epoch : 873 | Loss : 0.24880464375019073\n",
      "Epoch : 874 | Loss : 0.24806316196918488\n",
      "Epoch : 875 | Loss : 0.24732346832752228\n",
      "Epoch : 876 | Loss : 0.24658451974391937\n",
      "Epoch : 877 | Loss : 0.24584773182868958\n",
      "Epoch : 878 | Loss : 0.2451130598783493\n",
      "Epoch : 879 | Loss : 0.24438031017780304\n",
      "Epoch : 880 | Loss : 0.24364881217479706\n",
      "Epoch : 881 | Loss : 0.24291911721229553\n",
      "Epoch : 882 | Loss : 0.2421913594007492\n",
      "Epoch : 883 | Loss : 0.24146488308906555\n",
      "Epoch : 884 | Loss : 0.2407405823469162\n",
      "Epoch : 885 | Loss : 0.2400183230638504\n",
      "Epoch : 886 | Loss : 0.23929741978645325\n",
      "Epoch : 887 | Loss : 0.23857760429382324\n",
      "Epoch : 888 | Loss : 0.23786070942878723\n",
      "Epoch : 889 | Loss : 0.2371446043252945\n",
      "Epoch : 890 | Loss : 0.2364296168088913\n",
      "Epoch : 891 | Loss : 0.23571768403053284\n",
      "Epoch : 892 | Loss : 0.2350074201822281\n",
      "Epoch : 893 | Loss : 0.234297975897789\n",
      "Epoch : 894 | Loss : 0.23359069228172302\n",
      "Epoch : 895 | Loss : 0.2328849732875824\n",
      "Epoch : 896 | Loss : 0.2321808934211731\n",
      "Epoch : 897 | Loss : 0.23147879540920258\n",
      "Epoch : 898 | Loss : 0.230778306722641\n",
      "Epoch : 899 | Loss : 0.23007942736148834\n",
      "Epoch : 900 | Loss : 0.22938165068626404\n",
      "Epoch : 901 | Loss : 0.2286858707666397\n",
      "Epoch : 902 | Loss : 0.22799266874790192\n",
      "Epoch : 903 | Loss : 0.22730015218257904\n",
      "Epoch : 904 | Loss : 0.2266097068786621\n",
      "Epoch : 905 | Loss : 0.2259206920862198\n",
      "Epoch : 906 | Loss : 0.22523292899131775\n",
      "Epoch : 907 | Loss : 0.22454781830310822\n",
      "Epoch : 908 | Loss : 0.2238631546497345\n",
      "Epoch : 909 | Loss : 0.22318097949028015\n",
      "Epoch : 910 | Loss : 0.22250023484230042\n",
      "Epoch : 911 | Loss : 0.22182151675224304\n",
      "Epoch : 912 | Loss : 0.22114385664463043\n",
      "Epoch : 913 | Loss : 0.22046829760074615\n",
      "Epoch : 914 | Loss : 0.2197936475276947\n",
      "Epoch : 915 | Loss : 0.21912117302417755\n",
      "Epoch : 916 | Loss : 0.2184499204158783\n",
      "Epoch : 917 | Loss : 0.21778135001659393\n",
      "Epoch : 918 | Loss : 0.21711325645446777\n",
      "Epoch : 919 | Loss : 0.21644732356071472\n",
      "Epoch : 920 | Loss : 0.21578267216682434\n",
      "Epoch : 921 | Loss : 0.2151201367378235\n",
      "Epoch : 922 | Loss : 0.2144586592912674\n",
      "Epoch : 923 | Loss : 0.21379904448986053\n",
      "Epoch : 924 | Loss : 0.21314124763011932\n",
      "Epoch : 925 | Loss : 0.21248513460159302\n",
      "Epoch : 926 | Loss : 0.21182957291603088\n",
      "Epoch : 927 | Loss : 0.21117635071277618\n",
      "Epoch : 928 | Loss : 0.2105245441198349\n",
      "Epoch : 929 | Loss : 0.20987509191036224\n",
      "Epoch : 930 | Loss : 0.20922665297985077\n",
      "Epoch : 931 | Loss : 0.20858004689216614\n",
      "Epoch : 932 | Loss : 0.20793406665325165\n",
      "Epoch : 933 | Loss : 0.20729032158851624\n",
      "Epoch : 934 | Loss : 0.20664803683757782\n",
      "Epoch : 935 | Loss : 0.20600754022598267\n",
      "Epoch : 936 | Loss : 0.2053685188293457\n",
      "Epoch : 937 | Loss : 0.2047313153743744\n",
      "Epoch : 938 | Loss : 0.20409484207630157\n",
      "Epoch : 939 | Loss : 0.20346099138259888\n",
      "Epoch : 940 | Loss : 0.20282776653766632\n",
      "Epoch : 941 | Loss : 0.20219653844833374\n",
      "Epoch : 942 | Loss : 0.2015662044286728\n",
      "Epoch : 943 | Loss : 0.2009388953447342\n",
      "Epoch : 944 | Loss : 0.2003125101327896\n",
      "Epoch : 945 | Loss : 0.19968728721141815\n",
      "Epoch : 946 | Loss : 0.19906306266784668\n",
      "Epoch : 947 | Loss : 0.19844092428684235\n",
      "Epoch : 948 | Loss : 0.19782091677188873\n",
      "Epoch : 949 | Loss : 0.19720177352428436\n",
      "Epoch : 950 | Loss : 0.19658420979976654\n",
      "Epoch : 951 | Loss : 0.1959681212902069\n",
      "Epoch : 952 | Loss : 0.19535385072231293\n",
      "Epoch : 953 | Loss : 0.19474118947982788\n",
      "Epoch : 954 | Loss : 0.19412972033023834\n",
      "Epoch : 955 | Loss : 0.19351963698863983\n",
      "Epoch : 956 | Loss : 0.1929112821817398\n",
      "Epoch : 957 | Loss : 0.19230467081069946\n",
      "Epoch : 958 | Loss : 0.19169940054416656\n",
      "Epoch : 959 | Loss : 0.19109582901000977\n",
      "Epoch : 960 | Loss : 0.19049371778964996\n",
      "Epoch : 961 | Loss : 0.18989197909832\n",
      "Epoch : 962 | Loss : 0.18929311633110046\n",
      "Epoch : 963 | Loss : 0.1886950582265854\n",
      "Epoch : 964 | Loss : 0.18809868395328522\n",
      "Epoch : 965 | Loss : 0.1875031292438507\n",
      "Epoch : 966 | Loss : 0.18691064417362213\n",
      "Epoch : 967 | Loss : 0.1863180696964264\n",
      "Epoch : 968 | Loss : 0.18572784960269928\n",
      "Epoch : 969 | Loss : 0.18513856828212738\n",
      "Epoch : 970 | Loss : 0.18455132842063904\n",
      "Epoch : 971 | Loss : 0.18396514654159546\n",
      "Epoch : 972 | Loss : 0.1833810657262802\n",
      "Epoch : 973 | Loss : 0.18279758095741272\n",
      "Epoch : 974 | Loss : 0.18221570551395416\n",
      "Epoch : 975 | Loss : 0.18163561820983887\n",
      "Epoch : 976 | Loss : 0.18105675280094147\n",
      "Epoch : 977 | Loss : 0.18047931790351868\n",
      "Epoch : 978 | Loss : 0.17990373075008392\n",
      "Epoch : 979 | Loss : 0.1793292909860611\n",
      "Epoch : 980 | Loss : 0.17875586450099945\n",
      "Epoch : 981 | Loss : 0.17818449437618256\n",
      "Epoch : 982 | Loss : 0.17761556804180145\n",
      "Epoch : 983 | Loss : 0.17704837024211884\n",
      "Epoch : 984 | Loss : 0.17648178339004517\n",
      "Epoch : 985 | Loss : 0.17591632902622223\n",
      "Epoch : 986 | Loss : 0.17535313963890076\n",
      "Epoch : 987 | Loss : 0.1747911423444748\n",
      "Epoch : 988 | Loss : 0.17423082888126373\n",
      "Epoch : 989 | Loss : 0.17367179691791534\n",
      "Epoch : 990 | Loss : 0.17311422526836395\n",
      "Epoch : 991 | Loss : 0.17255759239196777\n",
      "Epoch : 992 | Loss : 0.17200228571891785\n",
      "Epoch : 993 | Loss : 0.1714489907026291\n",
      "Epoch : 994 | Loss : 0.17089667916297913\n",
      "Epoch : 995 | Loss : 0.17034600675106049\n",
      "Epoch : 996 | Loss : 0.1697964370250702\n",
      "Epoch : 997 | Loss : 0.16924913227558136\n",
      "Epoch : 998 | Loss : 0.16870242357254028\n",
      "Epoch : 999 | Loss : 0.16815701127052307\n"
     ]
    }
   ],
   "source": [
    "Train(a, b, 1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Saving the Trained Model ...\n",
    "torch.save(model, 'Model.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "SqrtNet(\n  (fc): Sequential(\n    (0): Linear(in_features=1, out_features=10, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=10, out_features=10, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=10, out_features=1, bias=True)\n  )\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Model\n",
    "model = torch.load('Model.pt')\n",
    "# Set the Model to Evaluation Mode\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12.2776],\n",
      "        [12.9066],\n",
      "        [25.8442],\n",
      "        [22.3175],\n",
      "        [31.2929]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "C = torch.tensor([[144], [160], [676], [500], [1000]])\n",
    "print(model(C))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
