{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Defining the NN\n",
    "class LinRegNet(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super(LinRegNet, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 100, 1]               2\n",
      "================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = LinRegNet()\n",
    "# Visualizing the NN\n",
    "summary(model, (100, 1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "X = torch.randn(100, 1)\n",
    "y = 5*X + 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def Train(input, target, EPOCH) :\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "    for index in range(EPOCH) :\n",
    "        output = model(input)\n",
    "        loss =  criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch : {index} | Loss : {loss.item()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 | Loss : 36.54094696044922\n",
      "Epoch : 1 | Loss : 34.98025894165039\n",
      "Epoch : 2 | Loss : 33.48637390136719\n",
      "Epoch : 3 | Loss : 32.05644226074219\n",
      "Epoch : 4 | Loss : 30.687715530395508\n",
      "Epoch : 5 | Loss : 29.377565383911133\n",
      "Epoch : 6 | Loss : 28.123483657836914\n",
      "Epoch : 7 | Loss : 26.923059463500977\n",
      "Epoch : 8 | Loss : 25.774003982543945\n",
      "Epoch : 9 | Loss : 24.674104690551758\n",
      "Epoch : 10 | Loss : 23.621257781982422\n",
      "Epoch : 11 | Loss : 22.613445281982422\n",
      "Epoch : 12 | Loss : 21.648740768432617\n",
      "Epoch : 13 | Loss : 20.725292205810547\n",
      "Epoch : 14 | Loss : 19.841333389282227\n",
      "Epoch : 15 | Loss : 18.99517059326172\n",
      "Epoch : 16 | Loss : 18.1851863861084\n",
      "Epoch : 17 | Loss : 17.409828186035156\n",
      "Epoch : 18 | Loss : 16.66761589050293\n",
      "Epoch : 19 | Loss : 15.957127571105957\n",
      "Epoch : 20 | Loss : 15.27700424194336\n",
      "Epoch : 21 | Loss : 14.625945091247559\n",
      "Epoch : 22 | Loss : 14.002703666687012\n",
      "Epoch : 23 | Loss : 13.406094551086426\n",
      "Epoch : 24 | Loss : 12.834970474243164\n",
      "Epoch : 25 | Loss : 12.28824234008789\n",
      "Epoch : 26 | Loss : 11.764870643615723\n",
      "Epoch : 27 | Loss : 11.263847351074219\n",
      "Epoch : 28 | Loss : 10.784221649169922\n",
      "Epoch : 29 | Loss : 10.325074195861816\n",
      "Epoch : 30 | Loss : 9.885531425476074\n",
      "Epoch : 31 | Loss : 9.464752197265625\n",
      "Epoch : 32 | Loss : 9.061935424804688\n",
      "Epoch : 33 | Loss : 8.676310539245605\n",
      "Epoch : 34 | Loss : 8.307141304016113\n",
      "Epoch : 35 | Loss : 7.9537272453308105\n",
      "Epoch : 36 | Loss : 7.615391254425049\n",
      "Epoch : 37 | Loss : 7.2914910316467285\n",
      "Epoch : 38 | Loss : 6.981407642364502\n",
      "Epoch : 39 | Loss : 6.684548854827881\n",
      "Epoch : 40 | Loss : 6.400351524353027\n",
      "Epoch : 41 | Loss : 6.128274440765381\n",
      "Epoch : 42 | Loss : 5.8677978515625\n",
      "Epoch : 43 | Loss : 5.618425369262695\n",
      "Epoch : 44 | Loss : 5.379683017730713\n",
      "Epoch : 45 | Loss : 5.151118278503418\n",
      "Epoch : 46 | Loss : 4.932294845581055\n",
      "Epoch : 47 | Loss : 4.722795486450195\n",
      "Epoch : 48 | Loss : 4.522223472595215\n",
      "Epoch : 49 | Loss : 4.33019495010376\n",
      "Epoch : 50 | Loss : 4.146348476409912\n",
      "Epoch : 51 | Loss : 3.970332145690918\n",
      "Epoch : 52 | Loss : 3.801811933517456\n",
      "Epoch : 53 | Loss : 3.640467882156372\n",
      "Epoch : 54 | Loss : 3.4859936237335205\n",
      "Epoch : 55 | Loss : 3.3380956649780273\n",
      "Epoch : 56 | Loss : 3.1964938640594482\n",
      "Epoch : 57 | Loss : 3.0609185695648193\n",
      "Epoch : 58 | Loss : 2.9311132431030273\n",
      "Epoch : 59 | Loss : 2.806831121444702\n",
      "Epoch : 60 | Loss : 2.6878366470336914\n",
      "Epoch : 61 | Loss : 2.573904514312744\n",
      "Epoch : 62 | Loss : 2.464818000793457\n",
      "Epoch : 63 | Loss : 2.3603715896606445\n",
      "Epoch : 64 | Loss : 2.260366201400757\n",
      "Epoch : 65 | Loss : 2.1646132469177246\n",
      "Epoch : 66 | Loss : 2.0729308128356934\n",
      "Epoch : 67 | Loss : 1.9851449728012085\n",
      "Epoch : 68 | Loss : 1.9010899066925049\n",
      "Epoch : 69 | Loss : 1.820607304573059\n",
      "Epoch : 70 | Loss : 1.7435444593429565\n",
      "Epoch : 71 | Loss : 1.669755220413208\n",
      "Epoch : 72 | Loss : 1.5991002321243286\n",
      "Epoch : 73 | Loss : 1.5314463376998901\n",
      "Epoch : 74 | Loss : 1.466665506362915\n",
      "Epoch : 75 | Loss : 1.404634952545166\n",
      "Epoch : 76 | Loss : 1.3452378511428833\n",
      "Epoch : 77 | Loss : 1.2883626222610474\n",
      "Epoch : 78 | Loss : 1.2339004278182983\n",
      "Epoch : 79 | Loss : 1.1817494630813599\n",
      "Epoch : 80 | Loss : 1.1318116188049316\n",
      "Epoch : 81 | Loss : 1.083992600440979\n",
      "Epoch : 82 | Loss : 1.0382016897201538\n",
      "Epoch : 83 | Loss : 0.9943522810935974\n",
      "Epoch : 84 | Loss : 0.9523626565933228\n",
      "Epoch : 85 | Loss : 0.9121527671813965\n",
      "Epoch : 86 | Loss : 0.8736476898193359\n",
      "Epoch : 87 | Loss : 0.8367746472358704\n",
      "Epoch : 88 | Loss : 0.8014644384384155\n",
      "Epoch : 89 | Loss : 0.7676501274108887\n",
      "Epoch : 90 | Loss : 0.7352689504623413\n",
      "Epoch : 91 | Loss : 0.7042587995529175\n",
      "Epoch : 92 | Loss : 0.674562394618988\n",
      "Epoch : 93 | Loss : 0.6461233496665955\n",
      "Epoch : 94 | Loss : 0.6188878417015076\n",
      "Epoch : 95 | Loss : 0.5928055047988892\n",
      "Epoch : 96 | Loss : 0.5678267478942871\n",
      "Epoch : 97 | Loss : 0.5439050197601318\n",
      "Epoch : 98 | Loss : 0.5209959745407104\n",
      "Epoch : 99 | Loss : 0.4990558326244354\n",
      "Epoch : 100 | Loss : 0.47804397344589233\n",
      "Epoch : 101 | Loss : 0.457920640707016\n",
      "Epoch : 102 | Loss : 0.4386481046676636\n",
      "Epoch : 103 | Loss : 0.4201899766921997\n",
      "Epoch : 104 | Loss : 0.4025120139122009\n",
      "Epoch : 105 | Loss : 0.3855814039707184\n",
      "Epoch : 106 | Loss : 0.3693663477897644\n",
      "Epoch : 107 | Loss : 0.3538363575935364\n",
      "Epoch : 108 | Loss : 0.3389621675014496\n",
      "Epoch : 109 | Loss : 0.3247162103652954\n",
      "Epoch : 110 | Loss : 0.31107190251350403\n",
      "Epoch : 111 | Loss : 0.2980036437511444\n",
      "Epoch : 112 | Loss : 0.2854868769645691\n",
      "Epoch : 113 | Loss : 0.27349787950515747\n",
      "Epoch : 114 | Loss : 0.26201513409614563\n",
      "Epoch : 115 | Loss : 0.25101664662361145\n",
      "Epoch : 116 | Loss : 0.24048219621181488\n",
      "Epoch : 117 | Loss : 0.2303919941186905\n",
      "Epoch : 118 | Loss : 0.22072704136371613\n",
      "Epoch : 119 | Loss : 0.2114696055650711\n",
      "Epoch : 120 | Loss : 0.20260250568389893\n",
      "Epoch : 121 | Loss : 0.19410884380340576\n",
      "Epoch : 122 | Loss : 0.18597321212291718\n",
      "Epoch : 123 | Loss : 0.17818020284175873\n",
      "Epoch : 124 | Loss : 0.17071527242660522\n",
      "Epoch : 125 | Loss : 0.16356459259986877\n",
      "Epoch : 126 | Loss : 0.15671496093273163\n",
      "Epoch : 127 | Loss : 0.1501537412405014\n",
      "Epoch : 128 | Loss : 0.14386866986751556\n",
      "Epoch : 129 | Loss : 0.13784795999526978\n",
      "Epoch : 130 | Loss : 0.13208059966564178\n",
      "Epoch : 131 | Loss : 0.1265559047460556\n",
      "Epoch : 132 | Loss : 0.1212635263800621\n",
      "Epoch : 133 | Loss : 0.11619357764720917\n",
      "Epoch : 134 | Loss : 0.1113368421792984\n",
      "Epoch : 135 | Loss : 0.10668407380580902\n",
      "Epoch : 136 | Loss : 0.10222689807415009\n",
      "Epoch : 137 | Loss : 0.09795696288347244\n",
      "Epoch : 138 | Loss : 0.09386631101369858\n",
      "Epoch : 139 | Loss : 0.08994743973016739\n",
      "Epoch : 140 | Loss : 0.08619297295808792\n",
      "Epoch : 141 | Loss : 0.08259601891040802\n",
      "Epoch : 142 | Loss : 0.0791502296924591\n",
      "Epoch : 143 | Loss : 0.07584893703460693\n",
      "Epoch : 144 | Loss : 0.072686105966568\n",
      "Epoch : 145 | Loss : 0.0696558728814125\n",
      "Epoch : 146 | Loss : 0.06675273180007935\n",
      "Epoch : 147 | Loss : 0.06397131085395813\n",
      "Epoch : 148 | Loss : 0.061306361109018326\n",
      "Epoch : 149 | Loss : 0.05875302478671074\n",
      "Epoch : 150 | Loss : 0.05630679056048393\n",
      "Epoch : 151 | Loss : 0.053962863981723785\n",
      "Epoch : 152 | Loss : 0.051717013120651245\n",
      "Epoch : 153 | Loss : 0.0495653972029686\n",
      "Epoch : 154 | Loss : 0.047503747045993805\n",
      "Epoch : 155 | Loss : 0.045528311282396317\n",
      "Epoch : 156 | Loss : 0.043635595589876175\n",
      "Epoch : 157 | Loss : 0.0418219119310379\n",
      "Epoch : 158 | Loss : 0.04008406773209572\n",
      "Epoch : 159 | Loss : 0.03841893747448921\n",
      "Epoch : 160 | Loss : 0.03682343661785126\n",
      "Epoch : 161 | Loss : 0.03529449924826622\n",
      "Epoch : 162 | Loss : 0.03382955119013786\n",
      "Epoch : 163 | Loss : 0.03242579475045204\n",
      "Epoch : 164 | Loss : 0.031080568209290504\n",
      "Epoch : 165 | Loss : 0.029791569337248802\n",
      "Epoch : 166 | Loss : 0.028556305915117264\n",
      "Epoch : 167 | Loss : 0.02737252227962017\n",
      "Epoch : 168 | Loss : 0.026238251477479935\n",
      "Epoch : 169 | Loss : 0.025151178240776062\n",
      "Epoch : 170 | Loss : 0.024109505116939545\n",
      "Epoch : 171 | Loss : 0.023111246526241302\n",
      "Epoch : 172 | Loss : 0.022154591977596283\n",
      "Epoch : 173 | Loss : 0.021237703040242195\n",
      "Epoch : 174 | Loss : 0.020359031856060028\n",
      "Epoch : 175 | Loss : 0.019517064094543457\n",
      "Epoch : 176 | Loss : 0.018710045143961906\n",
      "Epoch : 177 | Loss : 0.017936691641807556\n",
      "Epoch : 178 | Loss : 0.017195524647831917\n",
      "Epoch : 179 | Loss : 0.016485124826431274\n",
      "Epoch : 180 | Loss : 0.015804221853613853\n",
      "Epoch : 181 | Loss : 0.015151667408645153\n",
      "Epoch : 182 | Loss : 0.014526261016726494\n",
      "Epoch : 183 | Loss : 0.013926868326961994\n",
      "Epoch : 184 | Loss : 0.013352307491004467\n",
      "Epoch : 185 | Loss : 0.012801680713891983\n",
      "Epoch : 186 | Loss : 0.012273875065147877\n",
      "Epoch : 187 | Loss : 0.011768048629164696\n",
      "Epoch : 188 | Loss : 0.011283166706562042\n",
      "Epoch : 189 | Loss : 0.010818438604474068\n",
      "Epoch : 190 | Loss : 0.010372924618422985\n",
      "Epoch : 191 | Loss : 0.00994588527828455\n",
      "Epoch : 192 | Loss : 0.009536552242934704\n",
      "Epoch : 193 | Loss : 0.009144238196313381\n",
      "Epoch : 194 | Loss : 0.00876813568174839\n",
      "Epoch : 195 | Loss : 0.008407610468566418\n",
      "Epoch : 196 | Loss : 0.008061982691287994\n",
      "Epoch : 197 | Loss : 0.00773071451112628\n",
      "Epoch : 198 | Loss : 0.0074131363071501255\n",
      "Epoch : 199 | Loss : 0.00710872421041131\n",
      "Epoch : 200 | Loss : 0.006816915236413479\n",
      "Epoch : 201 | Loss : 0.0065371389500796795\n",
      "Epoch : 202 | Loss : 0.006268910598009825\n",
      "Epoch : 203 | Loss : 0.006011784076690674\n",
      "Epoch : 204 | Loss : 0.0057652597315609455\n",
      "Epoch : 205 | Loss : 0.005528932902961969\n",
      "Epoch : 206 | Loss : 0.00530239986255765\n",
      "Epoch : 207 | Loss : 0.005085189361125231\n",
      "Epoch : 208 | Loss : 0.004876947496086359\n",
      "Epoch : 209 | Loss : 0.004677325952798128\n",
      "Epoch : 210 | Loss : 0.004485896788537502\n",
      "Epoch : 211 | Loss : 0.004302408080548048\n",
      "Epoch : 212 | Loss : 0.004126457963138819\n",
      "Epoch : 213 | Loss : 0.003957774490118027\n",
      "Epoch : 214 | Loss : 0.003796044271439314\n",
      "Epoch : 215 | Loss : 0.0036409315653145313\n",
      "Epoch : 216 | Loss : 0.0034922203049063683\n",
      "Epoch : 217 | Loss : 0.00334966485388577\n",
      "Epoch : 218 | Loss : 0.0032129317987710238\n",
      "Epoch : 219 | Loss : 0.0030818558298051357\n",
      "Epoch : 220 | Loss : 0.00295614474453032\n",
      "Epoch : 221 | Loss : 0.00283560692332685\n",
      "Epoch : 222 | Loss : 0.002720053307712078\n",
      "Epoch : 223 | Loss : 0.0026092082262039185\n",
      "Epoch : 224 | Loss : 0.002502935007214546\n",
      "Epoch : 225 | Loss : 0.002401031320914626\n",
      "Epoch : 226 | Loss : 0.002303305547684431\n",
      "Epoch : 227 | Loss : 0.002209577476605773\n",
      "Epoch : 228 | Loss : 0.002119683427736163\n",
      "Epoch : 229 | Loss : 0.0020334995351731777\n",
      "Epoch : 230 | Loss : 0.0019508319674059749\n",
      "Epoch : 231 | Loss : 0.0018715395126491785\n",
      "Epoch : 232 | Loss : 0.001795508898794651\n",
      "Epoch : 233 | Loss : 0.0017226093914359808\n",
      "Epoch : 234 | Loss : 0.0016526748659089208\n",
      "Epoch : 235 | Loss : 0.0015856018289923668\n",
      "Epoch : 236 | Loss : 0.0015212747966870666\n",
      "Epoch : 237 | Loss : 0.001459594233892858\n",
      "Epoch : 238 | Loss : 0.0014004383701831102\n",
      "Epoch : 239 | Loss : 0.001343714538961649\n",
      "Epoch : 240 | Loss : 0.0012892796657979488\n",
      "Epoch : 241 | Loss : 0.0012370822951197624\n",
      "Epoch : 242 | Loss : 0.0011870093876495957\n",
      "Epoch : 243 | Loss : 0.0011389956343919039\n",
      "Epoch : 244 | Loss : 0.0010929385898634791\n",
      "Epoch : 245 | Loss : 0.0010487473336979747\n",
      "Epoch : 246 | Loss : 0.0010063714580610394\n",
      "Epoch : 247 | Loss : 0.0009657101472839713\n",
      "Epoch : 248 | Loss : 0.0009267253917641938\n",
      "Epoch : 249 | Loss : 0.000889316201210022\n",
      "Epoch : 250 | Loss : 0.0008534167427569628\n",
      "Epoch : 251 | Loss : 0.0008189869113266468\n",
      "Epoch : 252 | Loss : 0.0007859611650928855\n",
      "Epoch : 253 | Loss : 0.0007542746025137603\n",
      "Epoch : 254 | Loss : 0.0007238704711198807\n",
      "Epoch : 255 | Loss : 0.0006946970243006945\n",
      "Epoch : 256 | Loss : 0.0006667218403890729\n",
      "Epoch : 257 | Loss : 0.0006398720433935523\n",
      "Epoch : 258 | Loss : 0.0006141296471469104\n",
      "Epoch : 259 | Loss : 0.0005894238711334765\n",
      "Epoch : 260 | Loss : 0.0005657251458615065\n",
      "Epoch : 261 | Loss : 0.0005429728189483285\n",
      "Epoch : 262 | Loss : 0.0005211537936702371\n",
      "Epoch : 263 | Loss : 0.0005002210382372141\n",
      "Epoch : 264 | Loss : 0.00048013488412834704\n",
      "Epoch : 265 | Loss : 0.0004608657327480614\n",
      "Epoch : 266 | Loss : 0.00044237173278816044\n",
      "Epoch : 267 | Loss : 0.0004246229655109346\n",
      "Epoch : 268 | Loss : 0.00040760074625723064\n",
      "Epoch : 269 | Loss : 0.0003912571701221168\n",
      "Epoch : 270 | Loss : 0.0003755814104806632\n",
      "Epoch : 271 | Loss : 0.0003605461388360709\n",
      "Epoch : 272 | Loss : 0.0003461117739789188\n",
      "Epoch : 273 | Loss : 0.00033226492814719677\n",
      "Epoch : 274 | Loss : 0.00031897774897515774\n",
      "Epoch : 275 | Loss : 0.00030622497433796525\n",
      "Epoch : 276 | Loss : 0.0002939862897619605\n",
      "Epoch : 277 | Loss : 0.0002822469105012715\n",
      "Epoch : 278 | Loss : 0.0002709762775339186\n",
      "Epoch : 279 | Loss : 0.0002601583255454898\n",
      "Epoch : 280 | Loss : 0.00024977835710160434\n",
      "Epoch : 281 | Loss : 0.0002398069918854162\n",
      "Epoch : 282 | Loss : 0.00023024981783237308\n",
      "Epoch : 283 | Loss : 0.0002210738748544827\n",
      "Epoch : 284 | Loss : 0.00021226151147857308\n",
      "Epoch : 285 | Loss : 0.0002038114907918498\n",
      "Epoch : 286 | Loss : 0.00019569721189327538\n",
      "Epoch : 287 | Loss : 0.00018790944886859506\n",
      "Epoch : 288 | Loss : 0.00018043717136606574\n",
      "Epoch : 289 | Loss : 0.00017326039960607886\n",
      "Epoch : 290 | Loss : 0.00016636935470160097\n",
      "Epoch : 291 | Loss : 0.00015975184214767069\n",
      "Epoch : 292 | Loss : 0.0001534045732114464\n",
      "Epoch : 293 | Loss : 0.00014731756527908146\n",
      "Epoch : 294 | Loss : 0.00014146618195809424\n",
      "Epoch : 295 | Loss : 0.00013585093256551772\n",
      "Epoch : 296 | Loss : 0.00013046010280959308\n",
      "Epoch : 297 | Loss : 0.00012528705701697618\n",
      "Epoch : 298 | Loss : 0.0001203261490445584\n",
      "Epoch : 299 | Loss : 0.0001155599020421505\n"
     ]
    }
   ],
   "source": [
    "Train(X, y, 300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : tensor([[6.8300]]) | b : tensor([[37.1500]])\n"
     ]
    }
   ],
   "source": [
    "# Testing the Trained Model\n",
    "a = torch.tensor([[6.83]])\n",
    "b = 5*a + 3\n",
    "\n",
    "print(f'a : {a} | b : {b}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Saving the Model\n",
    "torch.save(model, 'Model.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[37.0778]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Load the Model\n",
    "model = torch.load('Model.pt')\n",
    "# Set the Model to Evaluation Mode\n",
    "model.eval()\n",
    "print(model(a))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
